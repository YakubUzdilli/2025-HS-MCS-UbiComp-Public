{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7420f93f",
   "metadata": {},
   "source": [
    "# (Pseudo-) Online Feature Calculation\n",
    "\n",
    "This notebook is used to receive data from the HoloLens 2. The data is chunked in 10 second pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a494ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fc384884",
   "metadata": {},
   "source": [
    "## Read the data from the newly arrived csv-file and call the feature calculation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "350e695e",
   "metadata": {},
   "source": [
    "### Run FeaturesCalculation notebook to make its function accessible here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cda22c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i FeatureCalculation.ipynb"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25237aa8",
   "metadata": {},
   "source": [
    "## Calculate the features and save them as csv\n",
    "See `FeaturesCalculation.ipynb`for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2ab4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features_for_10s_chunk(newdf):\n",
    "    list_of_features = []\n",
    "    newdf_valid = only_valid_data(newdf)\n",
    "    if (len(newdf_valid) > 1):\n",
    "        df_fixations = get_fixation_df(newdf_valid)\n",
    "        features = calculate_fixation_features(df_fixations, 10)\n",
    "        blinks = calculate_blink_features(newdf,10)\n",
    "        directions = calculate_directions_of_list(df_fixations)\n",
    "        density = calculate_fixation_density(newdf_valid, df_fixations)\n",
    "        features.update(blinks)\n",
    "        features.update(directions)\n",
    "        features.update(density)\n",
    "\n",
    "        # keep only the features the classifier expects\n",
    "        sig_feats = get_significant_features()\n",
    "        filtered = {k: features[k] for k in sig_feats if k in features}\n",
    "\n",
    "        filtered[\"label\"] = \"\"\n",
    "        filtered[\"duration\"] = \"10\"\n",
    "        filtered[\"participant_id\"] = \"002\"\n",
    "        list_of_features.append(filtered)\n",
    "\n",
    "        feature_file_path = save_as_csv(list_of_features, \"001\", './OnlineFeatureFiles/')\n",
    "        return feature_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b2bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_features(gaze_data_file_path):\n",
    "    # continue only if the csv-file contains data\n",
    "    if os.stat(gaze_data_file_path).st_size != 0:\n",
    "        df = pd.read_csv(gaze_data_file_path)\n",
    "        feature_file_path = calculate_features_for_10s_chunk(df)\n",
    "        print(f\"Feature calculation done for: {gaze_data_file_path}\")\n",
    "        print(f\"Feature file path: {feature_file_path}\")\n",
    "        return feature_file_path\n",
    "    else:\n",
    "        print(f\"File {gaze_data_file_path} is empty!\")\n",
    "        return \"\"\n",
    "# csv_to_features(\"./HL2_DataCollection/2022_09_23-13_41_19-Alex01-Inspection02.csv\")\n",
    "# uncomment the line above to test the function with a local csv file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01e6fc90",
   "metadata": {},
   "source": [
    "## Run SVM notebook to make its function accessible here\n",
    "If you are not training on normalized data this make take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528de9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "MODEL_PATH = \"./Models/extra_trees_classifier.joblib\"\n",
    "SCALER_PATH = \"./Models/feature_scaler.joblib\"\n",
    "\n",
    "if os.path.exists(MODEL_PATH) and os.path.exists(SCALER_PATH):\n",
    "    classifier = joblib.load(MODEL_PATH)\n",
    "    scaler = joblib.load(SCALER_PATH)\n",
    "    print(\"Loaded ExtraTrees model and scaler.\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"Model or scaler not found. Run AnExtraTreesClassifierForHL2GazeFeatures.ipynb first to create './Models/extra_trees_classifier.joblib' and './Models/feature_scaler.joblib'.\"\n",
    "    )\n",
    "\n",
    "def get_significant_features():\n",
    "    return [\n",
    "        \"freqDisPerSec\",\n",
    "        \"meanFix\",\n",
    "        \"maxFix\",\n",
    "        \"varFix\",\n",
    "        \"fixDensPerBB\",\n",
    "        \"blinkRate\",\n",
    "        \"meanDis\",\n",
    "        \"minDis\",\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9682dbc1",
   "metadata": {},
   "source": [
    "## Predict the class for the last arrived data chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d8857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_values(df):\n",
    "    # df: DataFrame with columns matching get_significant_features()\n",
    "    sig_feats = get_significant_features()\n",
    "    # enforce column order and presence\n",
    "    missing = [c for c in sig_feats if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Cannot normalize — missing features: {missing}\")\n",
    "    X_ordered = df[sig_feats].copy()\n",
    "    try:\n",
    "        X_scaled = scaler.transform(X_ordered)  # use saved StandardScaler\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Scaler transform failed. Check scaler and feature order. Details: {e}\")\n",
    "    scaled_df = pd.DataFrame(X_scaled, columns=sig_feats, index=df.index)\n",
    "    row_for_last_chunk = scaled_df.tail(1)\n",
    "    print(\"Normalized Features:\")\n",
    "    display(row_for_last_chunk)\n",
    "    return row_for_last_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2532e",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_counter = 0\n",
    "\n",
    "def predict_class_for_last_chunk(feature_file_path, prob_threshold=0.5):\n",
    "    global row_counter\n",
    "    if os.stat(feature_file_path).st_size == 0:\n",
    "        return\n",
    "\n",
    "    df = pd.read_csv(feature_file_path)\n",
    "    cur_number_of_rows = df.shape[0]\n",
    "\n",
    "    # only continue if we have a new row\n",
    "    if cur_number_of_rows <= row_counter:\n",
    "        return\n",
    "\n",
    "    last_row = df.tail(1)\n",
    "    display(last_row)\n",
    "\n",
    "    sig_feats = get_significant_features()\n",
    "    missing = [f for f in sig_feats if f not in last_row.columns]\n",
    "    if missing:\n",
    "        print(\"Missing features in feature file:\", missing)\n",
    "        row_counter = cur_number_of_rows\n",
    "        return\n",
    "\n",
    "    X = last_row[sig_feats]\n",
    "    X_norm = normalize_values(X)\n",
    "\n",
    "    pred_class = classifier.predict(X_norm)[0]\n",
    "    pred_proba = classifier.predict_proba(X_norm)[0]\n",
    "    max_prob = float(np.max(pred_proba))\n",
    "\n",
    "    print(f\"Predicted: {pred_class} (confidence {max_prob:.3f})\")\n",
    "\n",
    "    if max_prob >= prob_threshold:\n",
    "        send_activity(pred_class, max_prob)\n",
    "    else:\n",
    "        print(\"Confidence below threshold; not sent.\")\n",
    "\n",
    "    row_counter = cur_number_of_rows\n",
    "            \n",
    "# predict_class_for_last_chunk(\"FeatureFiles/feature_list_P09.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7abd846c",
   "metadata": {},
   "source": [
    "## Send the recognized activity back to the HoloLens 2 \n",
    "\n",
    "⚠️ Make sure to set the correct IP address for the HL2!\n",
    "\n",
    "If you run the app via Holographic remoting, this is your PC's IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7a32fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "import requests\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Remember to change the IP address and port if needed!\n",
    "# Start the HL2 app first, an input the url and port that you see in MR here\n",
    "holo_url = \"http://10.2.2.152:52739\"\n",
    "\n",
    "def send_activity(activity, probability):\n",
    "\n",
    "    url = \"{}/?activity={}&probability={}\".format(str(holo_url), str(activity), str(probability))\n",
    "    print(url)\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=120)\n",
    "        print(r)\n",
    "        if r.status_code == 200:\n",
    "            print(\"Notified Hololens about activity {}\".format(activity))\n",
    "        else:\n",
    "            print(\"Request {} failed with status code {}\".format(url, r.status_code))\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def create_and_send_test_data():\n",
    "    activities = [\"reading\", \"writing\", \"searching\", \"inspecting\"]\n",
    "    a = random.randint(0, 3)\n",
    "    activity = activities[a]\n",
    "    confidence = random.uniform(0, 1)\n",
    "    print(f\"{activity}: {confidence}\")\n",
    "    send_activity(activity, confidence)\n",
    "\n",
    "\n",
    "def sender():\n",
    "    start_time = time.time()\n",
    "    interval = 5\n",
    "    for i in range(20):\n",
    "        time.sleep(start_time + i * interval - time.time())\n",
    "        create_and_send_test_data()\n",
    "        print(\"sent data\")\n",
    "\n",
    "def send_this_desktop_ip_to_holo(desktop_ip, port):\n",
    "    \n",
    "    desktop_ip = urllib.parse.quote(desktop_ip)\n",
    "\n",
    "    url = \"{}/?desktopip={}&port={}\".format(str(holo_url), str(desktop_ip), str(port))\n",
    "    print(url)\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url, timeout=60)\n",
    "        print(r)\n",
    "        if r.status_code == 200:\n",
    "            print(\"Notified Hololens about IP address {}\".format(desktop_ip))\n",
    "        else:\n",
    "            print(\"Request {} failed with status code {}\".format(url, r.status_code))\n",
    "    except requests.ConnectionError as e:\n",
    "        print(\"OOPS!! Connection Error. Make sure you are connected to Internet. Technical Details given below.\\n\")\n",
    "        print(str(e))\n",
    "    except requests.Timeout as e:\n",
    "        print(\"OOPS!! Timeout Error\")\n",
    "        print(str(e))\n",
    "    except requests.RequestException as e:\n",
    "        print(\"OOPS!! General Error\")\n",
    "        print(str(e))\n",
    "\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "    # sender()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebc030f0",
   "metadata": {},
   "source": [
    "## Send this desktop's IP address to the HL2\n",
    "This enables the HL2 to send the gaze data correctly to the computer where this script is running.\n",
    "Make sure to enter your computer's IP-address and a free port."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca36ec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "\n",
    "def find_free_port():\n",
    "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
    "        s.bind(('', 0))  # 0 means to select an arbitrary unused port\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "free_port = find_free_port()\n",
    "print(f\"Free port found: {free_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048d8318",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipaddress = \"10.2.2.152\"\n",
    "# ipaddress = \"localhost\"\n",
    "# ipaddress = \"130.82.27.178\"\n",
    "send_this_desktop_ip_to_holo(ipaddress, free_port)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9268036",
   "metadata": {},
   "source": [
    "## Run a simple Flask server that receives the raw gaze data from the HL2\n",
    "Make sure to enter your computer's IP-address and a suitable port in the last line. \\\n",
    "Note: `0.0.0.0` as IP address lets the server listen on all its IP addresses, usually 127.0.0.1 and the public IP address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d07b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask_executor import Executor\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('waitress')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "app = Flask(__name__)\n",
    "executor = Executor(app)\n",
    "\n",
    "@app.route('/', methods=['POST', 'PUT'])\n",
    "def result():\n",
    "    new_csv = request.files[\"gazedata\"].read()\n",
    "    filename = request.form[\"filename\"]\n",
    "    print(f\"filename: {filename}\")\n",
    "    filepath = os.path.join(\"./OnlineGazeDataChunks/\", filename)\n",
    "    print(f\"filepath: {filepath}\")\n",
    "    outF = open(filepath, \"wb\")\n",
    "    outF.write(new_csv)\n",
    "    executor.submit(start_feature_calculation,filepath)\n",
    "    return 'Received !'  \n",
    "\n",
    "# '''\n",
    "def start_feature_calculation(filepath):\n",
    "    print(\"start fc for: \", filepath)\n",
    "    if os.stat(filepath).st_size != 0:\n",
    "        print(f\"File {filepath} is not empty, start feature calculation\")\n",
    "        feature_file_path = csv_to_features(filepath)\n",
    "        if os.stat(feature_file_path).st_size != 0:\n",
    "            print(\"start prediction for: \", feature_file_path)\n",
    "            predict_class_for_last_chunk(feature_file_path)\n",
    "        else:\n",
    "            print(f\"File {feature_file_path} is empty!\")\n",
    "    else:\n",
    "        print(f\"File {filepath} is empty!\")\n",
    "# '''\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from waitress import serve\n",
    "    serve(app, listen=f\"0.0.0.0:{free_port}\")\n",
    "    # serve(app, listen=\"10.2.2.152:5555\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
